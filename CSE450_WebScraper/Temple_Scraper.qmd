---
title: "CSE450 - Webscraping Tutorial"
subtitle: "October 18, 2023"
author: "Paul Cannon"
execute:
    warning: false
    keep-md: true
    df-print: paged
    enabled: true
format:
  html:
    theme: yeti
    embed-resources: true
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
---

# Hot off the presses

[Thoughts?](https://www.nature.com/articles/d41586-023-03254-5)


# Introduction

Webscraping is a great way for Data Scientists to gather data from useful websites.  Today, we're going to learn some of the basics of webscraping using Selenium.  Selenium is not the only way to webscrape, but it's a great place to start.  I also include here some resources to a library called "BeautifulSoup", which is another common way of pulling HTML files into Python.  

## Some Helpful Links:

[Beautiful Soup](https://stackabuse.com/guide-to-parsing-html-with-beautifulsoup-in-python/)

[Finding elements in Selenium](https://www.testim.io/blog/selenium-find-element-by-class/)

[Plotly Express](https://plotly.com/python/)

[PX Trendlines](https://plotly.com/python/linear-fits/)

[Datetimes](https://www.programiz.com/python-programming/datetime/strptime)

[RegEx](https://docs.python.org/3/library/re.html)

# Getting Started

Keep in mind a few best practices when accessing data from a website. 

## Good Practices
1. Do no harm
2. Check a website's "/robots.txt" at the root page
3. Use public API when available
4. Keep only relevant data
5. Just because it's accessible doesn't mean it should be accessed
6. Don't scrape private data
7. General Data Protection Regulation (GDPR)

## Load Libraries

```{python load_libraries}

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import string
import re
from datetime import datetime

```

## Establish a Web Driver

- A programming interface to create and execute test cases

[Temple Statistics]('https://churchofjesuschristtemples.org/statistics/')


```{python}

url = "https://churchofjesuschristtemples.org/statistics/milestones/"
driver =  webdriver.Chrome()
driver.get(url)


```

## Find the XPATH you're interested in (table elements, etc.)
Retrieve element(s) of interest using XPATH.  

What XPATH?

  - Location within an XML document

Why XPATH?

  - Dynamic webscraping
  - Helps find elements that are not found by other locators (class, name, etc)

How XPATH?

1. Try the whole table:

```{python}

table_info = driver.find_elements(By.XPATH, '//*[@id="locationTable"]/tbody/tr/td')

print(len(table_info))


list_data = [item.text for item in table_info]
print(list_data)


ncol = 4
nrow = int(len(list_data) / ncol)

```

```{python}
df = (pd.DataFrame(np.reshape(list_data, (nrow, ncol)), columns = ["temple", "announced", "gb", "ded"])
        # Filter out temples that haven't been dedicated yet
  .query('ded != " "')
        # create new columns that are dates formats
  .assign(
       ded_date = [datetime.strptime(date, "%d %B %Y") for date in df.ded],
       announced_date = [datetime.strptime(date, "%d %B %Y") for date in df.announced],
       ground_breaking_date = [datetime.strptime(date, "%d %B %Y") for date in df.gb],

       # The difference between 2 date variables doesn't return a number, so we need .dt.days to get the number of days
       n_days = lambda x: (x.ded_date - x.announced_date).dt.days,
       n_years = lambda x: x.n_days / 365,
       yr_announced = [datetime.strptime(date, "%d %B %Y") for date in df.gb],

       ## We can extract a year from a date variable like:
       year = lambda x: x.announced_date.dt.year
   )
)


```


# Plotly Express

A lot of this should look familiar if you've used altair.  Some slight differences, though.  I added a smoother trendline to make a higher level summary for the trends in years between announcement and dedication.  The Lowess smoother requires trendline_options for the parametrization.  There's obviously more customizations, but this should help get you on your way to making graphs in plotly express!

```{python}

new_chart = (px.scatter(
    df,
    x = "year",
    y = "n_years",
    color="temple",
    title = "How many years between announcement and dedication?",
    trendline="lowess",
    trendline_scope = "overall",
    trendline_options=dict(frac = .3),
    trendline_color_override="darkred",
    labels = {
      "year":"Year",
      "n_years":"Number of Years"
    }
)).update_layout(showlegend = False)

new_chart.show()

```


